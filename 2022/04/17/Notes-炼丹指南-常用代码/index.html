<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="未完待续  基本配置版本信息&#x2F;硬件信息12345678910import torchdef func():    print(torch.__version__)    print(torch.version.cuda)    print(torch.backends.cudnn.version())    print(torch.cuda.device_count())    print(to">
<meta property="og:type" content="article">
<meta property="og:title" content="Notes-炼丹指南-常用代码">
<meta property="og:url" content="http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/index.html">
<meta property="og:site_name" content="Yang Jing">
<meta property="og:description" content="未完待续  基本配置版本信息&#x2F;硬件信息12345678910import torchdef func():    print(torch.__version__)    print(torch.version.cuda)    print(torch.backends.cudnn.version())    print(torch.cuda.device_count())    print(to">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-04-17T11:44:08.000Z">
<meta property="article:modified_time" content="2022-04-17T14:44:19.971Z">
<meta property="article:author" content="yangjing">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon2.jpeg">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon2.jpeg" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon2.jpeg">
        
      
    
    <!-- title -->
    <title>Notes-炼丹指南-常用代码</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
      <link rel="alternate" href="/true" title="Yang Jing" type="application/atom+xml" />
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 6.0.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/recording/">Recording</a></li><!--
     --><!--
       --><li><a href="/links/">Links</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2022/09/06/Life-%E7%A7%8B%E6%8B%9B%E7%9A%84%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E7%82%BC%E4%B8%B9%E6%95%88%E7%8E%87/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&text=Notes-炼丹指南-常用代码"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&is_video=false&description=Notes-炼丹指南-常用代码"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Notes-炼丹指南-常用代码&body=Check out this article: http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&name=Notes-炼丹指南-常用代码&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&t=Notes-炼丹指南-常用代码"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE"><span class="toc-number">1.</span> <span class="toc-text">基本配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%88%E6%9C%AC%E4%BF%A1%E6%81%AF-%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF"><span class="toc-number">1.0.1.</span> <span class="toc-text">版本信息&#x2F;硬件信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.0.2.</span> <span class="toc-text">随机种子设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%98%BE%E5%8D%A1%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.0.3.</span> <span class="toc-text">显卡设置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">数据处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">2.</span> <span class="toc-text">基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%B4%E5%BA%A6%E4%BF%A1%E6%81%AF"><span class="toc-number">2.1.</span> <span class="toc-text">维度信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-number">2.2.</span> <span class="toc-text">数据转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%93%E4%B9%B1%E9%A1%BA%E5%BA%8F"><span class="toc-number">2.3.</span> <span class="toc-text">打乱顺序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%83%E7%B4%A0%E9%80%89%E6%8B%A9"><span class="toc-number">2.4.</span> <span class="toc-text">元素选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%A4%8D%E5%88%B6"><span class="toc-number">2.5.</span> <span class="toc-text">张量复制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">2.6.</span> <span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%8C%96"><span class="toc-number">2.7.</span> <span class="toc-text">维度变化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="toc-number">3.</span> <span class="toc-text">模型定义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">模型结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">3.2.</span> <span class="toc-text">模型可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">权重初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.4.</span> <span class="toc-text"></span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">4.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">4.1.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">4.2.</span> <span class="toc-text">测试</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">5.</span> <span class="toc-text">保存与加载</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">6.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-number">7.</span> <span class="toc-text">Reference</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Notes-炼丹指南-常用代码
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">yangjing</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2022-04-17T11:44:08.000Z" itemprop="datePublished">2022-04-17</time>
        
        (Updated: <time datetime="2022-04-17T14:44:19.971Z" itemprop="dateModified">2022-04-17</time>)
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <hr>
<p>未完待续</p>
<hr>
<h1 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h1><h3 id="版本信息-硬件信息"><a href="#版本信息-硬件信息" class="headerlink" title="版本信息/硬件信息"></a>版本信息/硬件信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">def func():</span><br><span class="line">    print(torch.__version__)</span><br><span class="line">    print(torch.version.cuda)</span><br><span class="line">    print(torch.backends.cudnn.version())</span><br><span class="line"></span><br><span class="line">    print(torch.cuda.device_count())</span><br><span class="line">    print(torch.cuda.get_device_name(0))</span><br><span class="line">    print(torch.cuda.current_device())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="随机种子设置"><a href="#随机种子设置" class="headerlink" title="随机种子设置"></a>随机种子设置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import random </span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def set_seed(seed):</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = True</span><br><span class="line">    torch.backends.cudnn.benchmark = False</span><br></pre></td></tr></table></figure>

<h3 id="显卡设置"><a href="#显卡设置" class="headerlink" title="显卡设置"></a>显卡设置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1 python train.py</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os </span><br><span class="line">os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0,1&#x27;</span><br></pre></td></tr></table></figure>

<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><h2 id="维度信息"><a href="#维度信息" class="headerlink" title="维度信息"></a>维度信息</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor.type()   # Data type</span><br><span class="line">tensor.dim() </span><br><span class="line">tensor.shape</span><br><span class="line">tensor.size()</span><br></pre></td></tr></table></figure>

<h2 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"># Type convertions.</span><br><span class="line">tensor = tensor.cuda()</span><br><span class="line">tensor = tensor.cpu()</span><br><span class="line">tensor = tensor.float()</span><br><span class="line">tensor = tensor.long()</span><br><span class="line">items = tensor.data()</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ndarray = tensor.cpu().numpy()</span><br><span class="line">tensor = torch.from_numpy(ndarray).float()</span><br><span class="line">tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.</span><br></pre></td></tr></table></figure>

<h2 id="打乱顺序"><a href="#打乱顺序" class="headerlink" title="打乱顺序"></a>打乱顺序</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor = tensor[torch.randperm(tensor.size(0))]  # 打乱第一个维度</span><br></pre></td></tr></table></figure>

<h2 id="元素选择"><a href="#元素选择" class="headerlink" title="元素选择"></a>元素选择</h2><p>非零元素</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nonzero(tensor)               # index of non-zero elements</span><br><span class="line">torch.nonzero(tensor==0)            # index of zero elements</span><br><span class="line">torch.nonzero(tensor).size(0)       # number of non-zero elements</span><br><span class="line">torch.nonzero(tensor == 0).size(0)  # number of zero elements</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.scatter_()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select()</span><br><span class="line">torch.index_fill()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.gather()</span><br></pre></td></tr></table></figure>



<h2 id="张量复制"><a href="#张量复制" class="headerlink" title="张量复制"></a>张量复制</h2><p>torch.detach() — 新的tensor会脱离计算图，不会牵扯梯度计算， torch.clone() — 新的tensor充当中间变量，会保留在计算图中，参与梯度计算（回传叠加），但是一般不会保留自身梯度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor.detach()</span><br><span class="line">tensor.detach().clone()</span><br></pre></td></tr></table></figure>

<p>张量相等</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.allclose(tensor1, tensor2)  # float tensor</span><br><span class="line">torch.equal(tensor1, tensor2)     # int tensor</span><br></pre></td></tr></table></figure>

<h2 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h2><h2 id="维度变化"><a href="#维度变化" class="headerlink" title="维度变化"></a>维度变化</h2><p>从功能上来看，view和reshape作用是相同的，都是用来重塑 Tensor 的 shape 的。</p>
<ul>
<li>view 只适合对满足连续性条件 (contiguous) 的 Tensor进行操作</li>
<li>而reshape 同时还可以对不满足连续性条件的 Tensor 进行操作，具有更好的鲁棒性。view 能干的 reshape都能干，如果 view 不能干就可以用 reshape 来处理<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Expand tensor of shape 64*512 to shape 64*512*7*7.</span><br><span class="line">tensor = torch.rand(64,512)</span><br><span class="line">torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)</span><br><span class="line">torch.view().contiguous()</span><br></pre></td></tr></table></figure>
维度转置<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.permutation()</span><br><span class="line">torch.transpose()</span><br></pre></td></tr></table></figure>
张量拼接<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，</span><br><span class="line">而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，</span><br><span class="line">而torch.stack的结果是3x10x5的张量。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">tensor = torch.cat(list_of_tensors, dim=0)</span><br><span class="line">tensor = torch.stack(list_of_tensors, dim=0)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># convolutional neural network (2 convolutional layers)</span><br><span class="line">class ConvNet(nn.Module):</span><br><span class="line">    def __init__(self, num_classes=10):</span><br><span class="line">        super(ConvNet, self).__init__()</span><br><span class="line">        self.layer1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),</span><br><span class="line">            nn.BatchNorm2d(16),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=2, stride=2))</span><br><span class="line">        self.layer2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),</span><br><span class="line">            nn.BatchNorm2d(32),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=2, stride=2))</span><br><span class="line">        self.fc = nn.Linear(7*7*32, num_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        out = self.layer1(x)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        out = out.reshape(out.size(0), -1)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = ConvNet(num_classes).to(device)</span><br></pre></td></tr></table></figure>

<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>计算模型参数量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())</span><br></pre></td></tr></table></figure>
<h2 id="模型可视化"><a href="#模型可视化" class="headerlink" title="模型可视化"></a>模型可视化</h2><ul>
<li>zagoruyko/pytorchvizgithub.com</li>
<li>sksq96/pytorch-summarygithub.com</li>
</ul>
<h2 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h2><p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Common practise for initialization.</span><br><span class="line">for layer in model.modules():</span><br><span class="line">    if isinstance(layer, torch.nn.Conv2d):</span><br><span class="line">        torch.nn.init.kaiming_normal_(layer.weight, mode=&#x27;fan_out&#x27;,</span><br><span class="line">                                      nonlinearity=&#x27;relu&#x27;)</span><br><span class="line">        if layer.bias is not None:</span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=0.0)</span><br><span class="line">    elif isinstance(layer, torch.nn.BatchNorm2d):</span><br><span class="line">        torch.nn.init.constant_(layer.weight, val=1.0)</span><br><span class="line">        torch.nn.init.constant_(layer.bias, val=0.0)</span><br><span class="line">    elif isinstance(layer, torch.nn.Linear):</span><br><span class="line">        torch.nn.init.xavier_normal_(layer.weight)</span><br><span class="line">        if layer.bias is not None:</span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=0.0)</span><br><span class="line"></span><br><span class="line"># Initialization with given tensor.</span><br><span class="line">layer.weight = torch.nn.Parameter(tensor)</span><br></pre></td></tr></table></figure>




<h2 id=""><a href="#" class="headerlink" title=""></a></h2><h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>基本框架</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Loss and optimizer</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"># Train the model</span><br><span class="line">total_step = len(train_loader)</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    for i ,(images, labels) in enumerate(train_loader):</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        # Forward pass</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        # Backward and optimizer</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        if (i+1) % 100 == 0:</span><br><span class="line">            print(&#x27;Epoch: [&#123;&#125;/&#123;&#125;], Step: [&#123;&#125;/&#123;&#125;], Loss: &#123;&#125;&#x27;</span><br><span class="line">                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))</span><br></pre></td></tr></table></figure>

<p>训练可视化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line">for n_iter in range(100):</span><br><span class="line">    writer.add_scalar(&#x27;Loss/train&#x27;, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(&#x27;Loss/test&#x27;, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(&#x27;Accuracy/train&#x27;, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(&#x27;Accuracy/test&#x27;, np.random.random(), n_iter)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>正则化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l1_regularization = torch.nn.L1Loss(reduction=&#x27;sum&#x27;)</span><br><span class="line">loss = ...  # Standard cross-entropy loss</span><br><span class="line">for param in model.parameters():</span><br><span class="line">    loss += torch.sum(torch.abs(param))</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>

<p>梯度裁剪</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)</span><br></pre></td></tr></table></figure>

<p>权重衰减(不对偏置项衰减)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bias_list = (param for name, param in model.named_parameters() if name[-4:] == &#x27;bias&#x27;)</span><br><span class="line">others_list = (param for name, param in model.named_parameters() if name[-4:] != &#x27;bias&#x27;)</span><br><span class="line">parameters = [&#123;&#x27;parameters&#x27;: bias_list, &#x27;weight_decay&#x27;: 0&#125;,                </span><br><span class="line">              &#123;&#x27;parameters&#x27;: others_list&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br></pre></td></tr></table></figure>

<p>学习率衰减</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Reduce learning rate when validation accuarcy plateau.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#x27;max&#x27;, patience=5, verbose=True)</span><br><span class="line">for t in range(0, 80):</span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br><span class="line">    scheduler.step(val_acc)</span><br><span class="line"></span><br><span class="line"># Cosine annealing learning rate.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)</span><br><span class="line"># Reduce learning rate by 10 at given epochs.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)</span><br><span class="line">for t in range(0, 80):</span><br><span class="line">    scheduler.step()    </span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br><span class="line"></span><br><span class="line"># Learning rate warmup by 10 epochs.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)</span><br><span class="line">for t in range(0, 10):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br></pre></td></tr></table></figure>
<p>微调全连接层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">for param in model.parameters():</span><br><span class="line">    param.requires_grad = False</span><br><span class="line">model.fc = nn.Linear(512, 100)  # Replace the last fc layer</span><br><span class="line">optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br></pre></td></tr></table></figure>

<p>分层学习率(大学习率微调全连接层，较小学习率微调卷积层)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">finetuned_parameters = list(map(id, model.fc.parameters()))</span><br><span class="line">conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)</span><br><span class="line">parameters = [&#123;&#x27;params&#x27;: conv_parameters, &#x27;lr&#x27;: 1e-3&#125;, </span><br><span class="line">              &#123;&#x27;params&#x27;: model.fc.parameters()&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>梯度累加</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>基本框架</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Test the model</span><br><span class="line">model.eval()  # eval mode(batch norm uses moving mean/variance </span><br><span class="line">              #instead of mini-batch mean/variance)</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    correct = 0</span><br><span class="line">    total = 0</span><br><span class="line">    for images, labels in test_loader:</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">        total += labels.size(0)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">    print(&#x27;Test accuracy of the model on the 10000 test images: &#123;&#125; %&#x27;.format(100 * correct / total))</span><br></pre></td></tr></table></figure>


<h1 id="保存与加载"><a href="#保存与加载" class="headerlink" title="保存与加载"></a>保存与加载</h1><p>同时保存模型和优化器的状态，以及当前的训练轮数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">start_epoch = 0</span><br><span class="line"># Load checkpoint.</span><br><span class="line">if resume: # resume为参数，第一次训练时设为0，中断再训练时设为1</span><br><span class="line">    model_path = os.path.join(&#x27;model&#x27;, &#x27;best_checkpoint.pth.tar&#x27;)</span><br><span class="line">    assert os.path.isfile(model_path)</span><br><span class="line">    checkpoint = torch.load(model_path)</span><br><span class="line">    best_acc = checkpoint[&#x27;best_acc&#x27;]</span><br><span class="line">    start_epoch = checkpoint[&#x27;epoch&#x27;]</span><br><span class="line">    model.load_state_dict(checkpoint[&#x27;model&#x27;])</span><br><span class="line">    optimizer.load_state_dict(checkpoint[&#x27;optimizer&#x27;])</span><br><span class="line">    print(&#x27;Load checkpoint at epoch &#123;&#125;.&#x27;.format(start_epoch))</span><br><span class="line">    print(&#x27;Best accuracy so far &#123;&#125;.&#x27;.format(best_acc))</span><br><span class="line"></span><br><span class="line"># Train the model</span><br><span class="line">for epoch in range(start_epoch, num_epochs): </span><br><span class="line">    ... </span><br><span class="line"></span><br><span class="line">    # Test the model</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    # save checkpoint</span><br><span class="line">    is_best = current_acc &gt; best_acc</span><br><span class="line">    best_acc = max(current_acc, best_acc)</span><br><span class="line">    checkpoint = &#123;</span><br><span class="line">        &#x27;best_acc&#x27;: best_acc,</span><br><span class="line">        &#x27;epoch&#x27;: epoch + 1,</span><br><span class="line">        &#x27;model&#x27;: model.state_dict(),</span><br><span class="line">        &#x27;optimizer&#x27;: optimizer.state_dict(),</span><br><span class="line">    &#125;</span><br><span class="line">    model_path = os.path.join(&#x27;model&#x27;, &#x27;checkpoint.pth.tar&#x27;)</span><br><span class="line">    best_model_path = os.path.join(&#x27;model&#x27;, &#x27;best_checkpoint.pth.tar&#x27;)</span><br><span class="line">    torch.save(checkpoint, model_path)</span><br><span class="line">    if is_best:</span><br><span class="line">        shutil.copy(model_path, best_model_path)</span><br></pre></td></tr></table></figure>
<p>保存</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save()</span><br></pre></td></tr></table></figure>

<p>加载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&#x27;model.pth&#x27;), strict=False)</span><br></pre></td></tr></table></figure>

<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul>
<li><p>不要使用太大的线性层。因为nn.Linear(m,n)使用的是O(mn)的内存，线性层太大很容易超出现有显存。</p>
</li>
<li><p>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</p>
</li>
<li><p>model(x) 前用 model.train() 和 model.eval() 切换网络状态。</p>
</li>
<li><p>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</p>
</li>
<li><p>model.eval() 和 torch.no_grad() 的区别在于</p>
<ul>
<li>model.eval() 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。</li>
<li>torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()。</li>
</ul>
</li>
<li><p>model.zero_grad()会把整个模型的参数的梯度都归零, 而optimizer.zero_grad()只会把传入其中的参数的梯度归零.</p>
</li>
<li><p>loss.backward() 前用 optimizer.zero_grad() 清除累积梯度。</p>
</li>
<li><p>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</p>
</li>
<li><p>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</p>
</li>
<li><p>使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</p>
</li>
<li><p>时常使用 assert tensor.size() == (N, D, H, W) 作为调试手段，确保张量维度和你设想中一致。</p>
</li>
<li><p>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</p>
</li>
<li><p>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息</p>
</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="http://karpathy.github.io/2019/04/25/recipe/">http://karpathy.github.io/2019/04/25/recipe/</a></li>
<li>张皓：PyTorch Cookbook（常用代码段整理合集），<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59205847">https://zhuanlan.zhihu.com/p/59205847</a>?</li>
<li>深度学习调参有哪些技巧？ - 知乎<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/25097993">https://www.zhihu.com/question/25097993</a></li>
<li>PyTorch官方文档和示例</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/faq.html">https://pytorch.org/docs/stable/notes/faq.html</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/szagoruyko/pytorchviz">https://github.com/szagoruyko/pytorchviz</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59205847">https://zhuanlan.zhihu.com/p/59205847</a></li>
<li><a target="_blank" rel="noopener" href="https://tsinghua-gongjing.github.io/posts/DL-tricks.html">https://tsinghua-gongjing.github.io/posts/DL-tricks.html</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56745640">https://zhuanlan.zhihu.com/p/56745640</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&amp;mid=2247519817&amp;idx=3&amp;sn=f484657f9adf12acce2c2f3f4dfaedd6&amp;key=60b42fe1b11cc8f78edcf8d1fa6f094d3db316540e56094b18b7b7f35ddf5e0b2bffc1783bf4731015787ab7cb1d20b120dac4015ca908c344b3205cc627906f088b8bee2e9bee2f0951bad0ae1c1ade&amp;ascene=1&amp;uin=MTQxOTUwNzc0MQ==&amp;devicetype=Windows+10&amp;version=62060739&amp;lang=zh_CN&amp;pass_ticket=ZLhErMmCBtNRK2zH29VMNwpzK/CnHDKQkK3+TAR8sNWTmgknQcDDEnFHckIZfMof">https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&amp;mid=2247519817&amp;idx=3&amp;sn=f484657f9adf12acce2c2f3f4dfaedd6&amp;key=60b42fe1b11cc8f78edcf8d1fa6f094d3db316540e56094b18b7b7f35ddf5e0b2bffc1783bf4731015787ab7cb1d20b120dac4015ca908c344b3205cc627906f088b8bee2e9bee2f0951bad0ae1c1ade&amp;ascene=1&amp;uin=MTQxOTUwNzc0MQ%3D%3D&amp;devicetype=Windows+10&amp;version=62060739&amp;lang=zh_CN&amp;pass_ticket=ZLhErMmCBtNRK2zH29VMNwpzK%2FCnHDKQkK3%2BTAR8sNWTmgknQcDDEnFHckIZfMof</a></li>
<li><a target="_blank" rel="noopener" href="http://karpathy.github.io/2019/04/25/recipe/">http://karpathy.github.io/2019/04/25/recipe/</a></li>
<li><a target="_blank" rel="noopener" href="https://jishuin.proginn.com/p/763bfbd5bbf6">https://jishuin.proginn.com/p/763bfbd5bbf6</a></li>
</ul>

  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/recording/">Recording</a></li>
         
          <li><a href="/links/">Links</a></li>
         
          <li><a href="/about/">About</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE"><span class="toc-number">1.</span> <span class="toc-text">基本配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%88%E6%9C%AC%E4%BF%A1%E6%81%AF-%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF"><span class="toc-number">1.0.1.</span> <span class="toc-text">版本信息&#x2F;硬件信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.0.2.</span> <span class="toc-text">随机种子设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%98%BE%E5%8D%A1%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.0.3.</span> <span class="toc-text">显卡设置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">数据处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">2.</span> <span class="toc-text">基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%B4%E5%BA%A6%E4%BF%A1%E6%81%AF"><span class="toc-number">2.1.</span> <span class="toc-text">维度信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-number">2.2.</span> <span class="toc-text">数据转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%93%E4%B9%B1%E9%A1%BA%E5%BA%8F"><span class="toc-number">2.3.</span> <span class="toc-text">打乱顺序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%83%E7%B4%A0%E9%80%89%E6%8B%A9"><span class="toc-number">2.4.</span> <span class="toc-text">元素选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%A4%8D%E5%88%B6"><span class="toc-number">2.5.</span> <span class="toc-text">张量复制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">2.6.</span> <span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%8C%96"><span class="toc-number">2.7.</span> <span class="toc-text">维度变化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="toc-number">3.</span> <span class="toc-text">模型定义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">模型结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">3.2.</span> <span class="toc-text">模型可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">权重初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.4.</span> <span class="toc-text"></span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">4.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">4.1.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">4.2.</span> <span class="toc-text">测试</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">5.</span> <span class="toc-text">保存与加载</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">6.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-number">7.</span> <span class="toc-text">Reference</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&text=Notes-炼丹指南-常用代码"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&is_video=false&description=Notes-炼丹指南-常用代码"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Notes-炼丹指南-常用代码&body=Check out this article: http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&title=Notes-炼丹指南-常用代码"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&name=Notes-炼丹指南-常用代码&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2022/04/17/Notes-%E7%82%BC%E4%B8%B9%E6%8C%87%E5%8D%97-%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/&t=Notes-炼丹指南-常用代码"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2022
    yangjing
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/recording/">Recording</a></li><!--
     --><!--
       --><li><a href="/links/">Links</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = 'yangjingla/comments';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'Comment';
      var utterances_theme = 'github-light';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
